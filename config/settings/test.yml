# These settings are merged into config/settings.yml

# https://aws.amazon.com/sdk-for-ruby/
#
# Some image-ids as of 2017-08
# Amazon Linux AMI 2017.03.1 (HVM), SSD Volume Type - ami-aa5ebdd2
# Red Hat Enterprise Linux 7.4 (HVM), SSD Volume Type - ami-9fa343e7
# Ubuntu Server 16.04 LTS (HVM), SSD Volume Type - ami-6e1a0117

# - these test settings may use small instances that cannot support large
#   application loads
# - these test settings may not be fault tolerant
# - these settings may consolidate services on single nodes that should be replicated
#   on multiple nodes in different availability zones for production fault tolerance

# IMPORTANT - Get an AWS Key Pair from the AWS EC2 console and use the name of that key
#             in the `key_name` field for the instance default settings.

# IMPORTANT - Find an AMI image_id and set it, ensure the OS on that AMI matches the
#             value in `Distributor ID:`, which can be found from the
#             `lsb_release -a`.  This test setting is using an Ubuntu AMI.
#             This can make a difference for package installations.
#
# It might be possible to dynamically detect the OS, e.g.
# https://unix.stackexchange.com/questions/6345/how-can-i-get-distribution-name-and-version-number-in-a-simple-shell-script

aws:
  # when access_key_id and secret_access_key are blank, Settings will try to use ENV values
  access_key_id:
  secret_access_key:
  region: us-west-2
  availability_zones:
    - us-west-2a
    - us-west-2b
    - us-west-2c

  instance_default_test: &TestDefault
    resource: instance
    region: us-west-2
    image_id: ami-6e1a0117  # Ubuntu
    distrib_id: Ubuntu
    user: ubuntu
    min_count: 1
    max_count: 1
    instance_type: t2.micro
    availability_zone: us-west-2a
    security_groups:
      - test_ssh_security_group
    key_name: dweber-consulting-test
    tag_name: test_default
    tag_group: test_nodes
    tag_manager: dlweber
    tag_service:
    tag_stage: test

  # ---
  # Zookeeper Nodes
  # - note definition and use of defaults: ZookeeperDefaults
  # - it's important that each instance has 'zookeeper' in the key and tag_name (case sensitive)
  # - it's important that each instance has a 'myid' with a unique integer between 1-255 (inclusive)
  # - the `leader_port` and the `election_port` must match up with security_group settings

  test_zookeeper_configuration:
    resource: configuration
    maxClientCnxns: 0
    client_port: 2181
    leader_port: 2888
    election_port: 3888
    dataLogDir: "/var/log/zookeeper"
    dataDir: "/var/lib/zookeeper"

  test_zookeeper1: &ZookeeperDefaults
    <<: *TestDefault
    myid: 1
    client_port: 2181
    leader_port: 2888
    election_port: 3888
    security_groups:
      - test_ssh_security_group
      - test_zookeeper_security_group
    tag_service: zookeeper
    tag_group: test_zookeeper
    tag_name: test_zookeeper1
    availability_zone: us-west-2a  # need one node in each zone

  test_zookeeper2:
    <<: *ZookeeperDefaults
    myid: 2
    tag_name: test_zookeeper2
    availability_zone: us-west-2b  # need one node in each zone

  test_zookeeper3:
    <<: *ZookeeperDefaults
    myid: 3
    tag_name: test_zookeeper3
    availability_zone: us-west-2c  # need one node in each zone

  # ---
  # Kafka Nodes
  # See also https://www.confluent.io/blog/design-and-deployment-considerations-for-deploying-apache-kafka-on-aws/
  # - note definition and use of defaults: KafkaDefaults
  # - it's important that each instance has 'kafka' in the key and tag_name (case sensitive)

  # See server.properties for details
  # - settings that are handled dynamically include:
  #   - broker.id
  #   - zookeeper.connect
  #   - advertised.listeners
  test_kafka_configuration:
    resource: configuration
    kafka_home: "/opt/kafka" # symlinks to /opt/kafka_{SCALA_VERSION}-{KAFKA_VERSION}
    kafka_version: "0.11.0.0"
    scala_version: "2.11"
    # Note on a t1.micro we need KAFKA_HEAP_OPTS="-Xmx512M -Xms512M"
    kafka_heap_opts: "-Xmx512M -Xms512M"  # defaults to "-Xmx1G -Xms1G"
    delete.topic.enable: true # use 'false' in production
    auto.create.topics.enable: true # use 'false' in production
    log.dirs: /data/kafka  # CSV, best to match N log.dirs to partitions
    num.partitions: 8
    default.replication.factor: 3
    min.insync.replicas: 2
    log.retention.hours: 168
    log.segment.bytes: 1073741824
    log.retention.check.interval.ms: 300000
    zookeeper.connection.timeout.ms: 6000

  test_kafka1: &KafkaDefaults
    <<: *TestDefault
    broker_id: 1
    client_port: 9092 # AFAIK, it's always 9092
    tag_service: kafka
    tag_group: test_kafka
    tag_name: test_kafka1
    availability_zone: us-west-2a  # need one node in each zone
    security_groups:
      - test_ssh_security_group
      - test_kafka_security_group

  test_kafka2:
    <<: *KafkaDefaults
    broker_id: 2
    tag_name: test_kafka2
    availability_zone: us-west-2b  # need one node in each zone

  test_kafka3:
    <<: *KafkaDefaults
    broker_id: 3
    tag_name: test_kafka3
    availability_zone: us-west-2c  # need one node in each zone

  # ---
  # Mesos Nodes
  # - note definition and use of defaults: MesosMasterDefaults, MesosAgentDefaults
  # - it's important that each instance has 'mesos' in the key and tag_name (case sensitive)

  test_mesos_master1: &MesosMasterDefaults
    <<: *TestDefault
    tag_service: mesos
    tag_group: test_mesos_master
    tag_name: test_mesos_master1
    availability_zone: us-west-2a  # need one node in each zone

  test_mesos_master2:
    <<: *MesosMasterDefaults
    tag_name: test_mesos_master2
    availability_zone: us-west-2b  # need one node in each zone

  test_mesos_master3:
    <<: *MesosMasterDefaults
    tag_name: test_mesos_master3
    availability_zone: us-west-2c  # need one node in each zone

  test_mesos_agent1: &MesosAgentDefaults
    <<: *TestDefault
    tag_service: mesos
    tag_group: test_mesos_agent
    tag_name: test_mesos_agent1
    availability_zone: us-west-2a  # need one node in each zone

  test_mesos_agent2:
    <<: *MesosAgentDefaults
    tag_name: test_mesos_agent2
    availability_zone: us-west-2b  # need one node in each zone

  test_mesos_agent3:
    <<: *MesosAgentDefaults
    tag_name: test_mesos_agent3
    availability_zone: us-west-2c  # need one node in each zone

  # ---
  # Security Groups
  # - a 'Default VPC' can be found on the EC2 Dashboard
  # - the `group_id` is dynamically assigned and should be blank here
  # - creation of security groups is idempotent (AWS doesn't allow duplicates)
  # - instance creation can validate or create a new security group, as necessary

  test_security_group_defaults: &TestSecurityGroupDefault
    resource: security_group
    vpc_id: vpc-14d0be7c
    tag_group: test_security_groups
    tag_manager: dlweber
    tag_stage: test

  test_ssh_security_group:
    <<: *TestSecurityGroupDefault
    tag_name: test_ssh_security_group
    group_name: test_ssh_security_group
    description: 'SSH access'
    authorize_ingress:
      group_id:
      ip_permissions:
        - ip_protocol: tcp
          from_port: 22
          to_port: 22
          ip_ranges:
            - cidr_ip: '0.0.0.0/0'

  # - these ip_permissions for the test stage are wide open, for ease of use
  # - for production, the IP CIDR needs to be specific to subnets
  # - the public subnets should have an IPV4 CIDR for each availability_zone
  test_zookeeper_security_group:
    <<: *TestSecurityGroupDefault
    tag_name: test_zookeeper_security_group
    group_name: test_zookeeper_security_group
    description: 'Zookeeper port access'
    authorize_ingress:
      group_id:
      ip_permissions:
        - ip_protocol: tcp
          from_port: 2181
          to_port: 2181
          ip_ranges:
            - cidr_ip: '0.0.0.0/0'
          ipv_6_ranges:
            - cidr_ipv_6: '::/0'
        - ip_protocol: tcp
          from_port: 2888
          to_port: 2888
          ip_ranges:
            - cidr_ip: '0.0.0.0/0'
          ipv_6_ranges:
            - cidr_ipv_6: '::/0'
        - ip_protocol: tcp
          from_port: 3888
          to_port: 3888
          ip_ranges:
            - cidr_ip: '0.0.0.0/0'
          ipv_6_ranges:
            - cidr_ipv_6: '::/0'
        - ip_protocol: tcp
          from_port: 8001  # see lib/zoonavigator/zoonavigator-docker-compose.yml
          to_port: 8001
          ip_ranges:
            - cidr_ip: '0.0.0.0/0'
          ipv_6_ranges:
            - cidr_ipv_6: '::/0'

  # - these ip_permissions for the test stage are wide open, for ease of use
  # - for production, the IP CIDR needs to be specific to subnets
  # - the public subnets should have an IPV4 CIDR for each availability_zone
  test_kafka_security_group:
    <<: *TestSecurityGroupDefault
    tag_name: test_kafka_security_group
    group_name: test_kafka_security_group
    description: 'Kafka port access'
    authorize_ingress:
      group_id:
      ip_permissions:
        - ip_protocol: tcp
          from_port: 9092
          to_port: 9092
          ip_ranges:
            - cidr_ip: '0.0.0.0/0'
          ipv_6_ranges:
            - cidr_ipv_6: '::/0'
